{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python - Web interaction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. The Internet\n",
    "\n",
    "Each device has a globally distinct IP address, which is a 32 bit number. Usually an IP address is represented as a sequence of four decimal numbers, each number in the range (0, 255). For example, when I checked the IP address for my laptop just now, it was 141.211.203.248. Any IP address beginning with 141.211 is for a device at the University of Michigan. When I take my laptop home and connect to a network there, my laptop gets a different IP address that it uses there.    \n",
    "\n",
    "Data is chopped up into reasonable sized packets (up to 65,535 bytes, but usually much smaller).     \n",
    "\n",
    "Each data packet has a header that includes the destination IP address.     \n",
    "\n",
    "Each packet is routed independently, getting passed on from one computing device to another until it reaches its destination. The computing devices that do that packet forwarding are called routers. Each router keeps an address table that says, when it gets a packet for some destination address, which of its neighbors should it pass the packet on to. The routers are constantly talking to each other passing information about how they should update their routing tables. The system was designed to be resistant to any local damage. If some of the routers stop working, the rest of the routers talk to each other and start routing packets around in a different way so that packets still reach their intended destination if there is some path to get there. It is this technical capability that has spawned metaphoric quotes like this one from John Gilmore: “The Net interprets censorship as damage and routes around it.”       \n",
    "\n",
    "At the destination, the packets are reassembled into the original data message."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1  URL\n",
    "\n",
    "A URL is used by a browser or other program to specify what server to connect to and what page to ask for. Like other things that will be interpreted by computer programs, URLs have a very specific formal structure. If you put a colon in the wrong place, the URL won’t work correctly. The overall structure of a URL is:       \n",
    "\n",
    "(scheme)://(host):(port)/(path)\n",
    "    \n",
    "Usually, the scheme will be http or https. The s in https stands for “secure”. When you use https, all of the communication between the two devices is encrypted. Any devices that intercepts some of the packets along the way will be unable to decrypt the contents and figure out what the data was.        \n",
    "\n",
    "Other schemes that you will sometimes see include ftp (for file transfer) and mailto (for email addresses).        \n",
    "\n",
    "The host will usually be a domain name, like si.umich.edu or github.com or google.com. When the URL specifies a domain name, the first thing the computer program does is look up the domain name to find the 32-bit IP address. For example, right now the IP adddress for github.com is 192.30.252.130. This could change if, for example, github moved its servers to a different location or contracted with a different Internet provider. Lookups use something called the Domain Name System, or DNS for short. Changes to the mapping from domain names to IP addresses can take a little while to propagate: if github.com announces a new IP address associated with its domain, it might take up to 24 hours for some computers to start translating github.com to the new IP address.         \n",
    "\n",
    "Alternatively, the host can be an IP address directly. This is less common, because IP addresses are harder to remember and because a URL containing a domain name will continue to work even if the remote server keeps its domain name but moves to a different IP address.          \n",
    "\n",
    "The :port is optional. If it is omitted, the default port number is 80. The port number is used on the receiving end to decide which computer program should get the data that has been received. We probably will not encounter any URLs that include the : and a port number in this course.           \n",
    "\n",
    "The /path is also optional. It specifies something about which page, or more generally which contents, are being requested.       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. *urllib* library\n",
    "\n",
    "This package provides interface for fetching data from the World Wide Web.    \n",
    "*urlopen()* accepts URLs instead of file names.   \n",
    "\n",
    "URL: Uniform/Universal Resource Locator   \n",
    "Vast majority of the URLs are web addresses, but can also be File Transfer Protocol (FTP) or database access.    \n",
    "URLs that are web addresses or location of the websites consist of 2 parts:   \n",
    "1) Protocol identifier: http or https   \n",
    "2) Resource name: datacamp.com   \n",
    "\n",
    "HTTP: HyperText Transfer Protocol   \n",
    "Going to a website: sending HTTP request to a server, this is known as **GET** request, most common type of HTTP request.   \n",
    "*urlretrieve()* performs a **GET** request and also saves the relevant data locally.   \n",
    "\n",
    "\n",
    "HTML: Hypertext Markup Language   \n",
    "Important: tags which tells you where hyperlink or headings can be found.   \n",
    "You need to parse and extract useful data. (e.g. BeautifulSoup package)     \n",
    "\n",
    "\n",
    "Behind the scene of a GET request:     \n",
    "1) my computer convert domain name into an IP address (target IP address)     \n",
    "2) my computer makes connection to the remote computer (if https is used, then my computer will set up encryption keys) with the remote computer first)     \n",
    "3) my computer send message using the http protocol    \n",
    "3.1) GET {arguments}  (recall that the format is http://(server/domain name)/(arguments). It will also send some headers (info like here is chrome browser, timestamp etc, the format of the argument is *server specific*)     \n",
    "3.2) We will receive back some header from the server (remote computer) (headers contain stuff like I am sending you back some HTML and also time stamp)      \n",
    "3.3) my browser will turn that HTML into webpage.       \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Automate file download in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#from urllib.request import urlretrieve #urlretrieve is a function we need\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('winequality_white.csv', <http.client.HTTPMessage at 0x110fb4208>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv\"\n",
    "urllib.request.urlretrieve(url, \"winequality_white.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
      "0            7.0              0.27         0.36            20.7      0.045   \n",
      "1            6.3              0.30         0.34             1.6      0.049   \n",
      "2            8.1              0.28         0.40             6.9      0.050   \n",
      "3            7.2              0.23         0.32             8.5      0.058   \n",
      "4            7.2              0.23         0.32             8.5      0.058   \n",
      "\n",
      "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
      "0                 45.0                 170.0   1.0010  3.00       0.45   \n",
      "1                 14.0                 132.0   0.9940  3.30       0.49   \n",
      "2                 30.0                  97.0   0.9951  3.26       0.44   \n",
      "3                 47.0                 186.0   0.9956  3.19       0.40   \n",
      "4                 47.0                 186.0   0.9956  3.19       0.40   \n",
      "\n",
      "   alcohol  quality  \n",
      "0      8.8        6  \n",
      "1      9.5        6  \n",
      "2     10.1        6  \n",
      "3      9.9        6  \n",
      "4      9.9        6  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"winequality_white.csv\", sep=\";\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Direct import files from web without save locally (Pandas) (CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt #plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
      "0            7.0              0.27         0.36            20.7      0.045   \n",
      "1            6.3              0.30         0.34             1.6      0.049   \n",
      "2            8.1              0.28         0.40             6.9      0.050   \n",
      "3            7.2              0.23         0.32             8.5      0.058   \n",
      "4            7.2              0.23         0.32             8.5      0.058   \n",
      "\n",
      "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
      "0                 45.0                 170.0   1.0010  3.00       0.45   \n",
      "1                 14.0                 132.0   0.9940  3.30       0.49   \n",
      "2                 30.0                  97.0   0.9951  3.26       0.44   \n",
      "3                 47.0                 186.0   0.9956  3.19       0.40   \n",
      "4                 47.0                 186.0   0.9956  3.19       0.40   \n",
      "\n",
      "   alcohol  quality  \n",
      "0      8.8        6  \n",
      "1      9.5        6  \n",
      "2     10.1        6  \n",
      "3      9.9        6  \n",
      "4      9.9        6  \n"
     ]
    }
   ],
   "source": [
    "url = \"http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv\"\n",
    "df3 = pd.read_csv(url, sep=\";\") #just read into DataFrames\n",
    "print(df3.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot first column of df\n",
    "pd.DataFrame.hist(df.iloc[:, 0:1])\n",
    "plt.xlabel('fixed acidity (g(tartaric acid)/dm$^3$)')\n",
    "plt.ylabel('count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Direct import files from web without save locally (Pandas) (Excel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['1700', '1900'])\n",
      "                 country       1700\n",
      "0            Afghanistan  34.565000\n",
      "1  Akrotiri and Dhekelia  34.616667\n",
      "2                Albania  41.312000\n",
      "3                Algeria  36.720000\n",
      "4         American Samoa -14.307000\n"
     ]
    }
   ],
   "source": [
    "url = \"http://s3.amazonaws.com/assets.datacamp.com/course/importing_data_into_r/latitude.xls\"\n",
    "xl = pd.read_excel(url, sheet_name= None) #read all sheets into xl, note that xl is a dictionary with key: sheet_name and value is the DataFrame of the corresponding sheet\n",
    "\n",
    "# Print the sheetnames to the shell\n",
    "print(xl.keys())\n",
    "\n",
    "# Print the head of the first sheet (using its name, NOT its index)\n",
    "print(xl[\"1700\"].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 *GET* request using urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'http.client.HTTPResponse'>\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen, Request\n",
    "url = \"https://www.wikipedia.org/\"\n",
    "request= Request(url) # This packages the request: request\n",
    "response = urlopen(request) # Sends the request and catches the response: response\n",
    "print(type(response))\n",
    "#html = response.read()\n",
    "response.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 *requests* package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.wikipedia.org/\"\n",
    "r = requests.get(url)\n",
    "text = r.text #returns HTML as a string\n",
    "#print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 *BeautifulSoup*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6.1 Prettify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.0 Transitional//EN\" \"http://www.w3.org/TR/REC-html40/transitional.dtd\">\n",
      "<html>\n",
      " <head>\n",
      "  <meta content=\"text/html; charset=utf-8\" http-equiv=\"Content-Type\"/>\n",
      "  <title>\n",
      "   Beautiful Soup: We called him Tortoise because he taught us.\n",
      "  </title>\n",
      "  <link href=\"mailto:leonardr@segfault.org\" rev=\"made\"/>\n",
      "  <link href=\"/nb/themes/Default/nb.css\" rel=\"stylesheet\" type=\"text/css\"/>\n",
      "  <meta content=\"Beautiful Soup: a library designed for screen-scraping HTML and XML.\" name=\"Description\"/>\n",
      "  <meta content=\"Markov Approximation 1.4 (module: leonardr)\" name=\"generator\"/>\n",
      "  <meta content=\"Leonard Richardson\" name=\"author\"/>\n",
      " </head>\n",
      " <body alink=\"red\" bgcolor=\"white\" link=\"blue\" text=\"black\" vlink=\"660066\">\n",
      "  <img align=\"right\" src=\"10.1.jpg\" width=\"250\"/>\n",
      "  <br/>\n",
      "  <p>\n",
      "   You didn't write that awful page. You're just trying to get some\n",
      "data out of it. Beautiful Soup is here to help. Since 2004, it's been\n",
      "saving programmers hours or days of work on quick-turnaround\n",
      "screen scraping projects.\n",
      "  </p>\n",
      "  <div align=\"center\">\n",
      "   <a href=\"bs4/download/\">\n",
      "    <h1>\n",
      "     Beautiful Soup\n",
      "    </h1>\n",
      "   </a>\n",
      "   <p>\n",
      "    \"A tremendous boon.\" -- Python411 Podcast\n",
      "   </p>\n",
      "   <p>\n",
      "    [\n",
      "    <a href=\"#Download\">\n",
      "     Download\n",
      "    </a>\n",
      "    |\n",
      "    <a href=\"bs4/doc/\">\n",
      "     Documentation\n",
      "    </a>\n",
      "    |\n",
      "    <a href=\"#HallOfFame\">\n",
      "     Hall of Fame\n",
      "    </a>\n",
      "    |\n",
      "    <a href=\"https://code.launchpad.net/beautifulsoup\">\n",
      "     Source\n",
      "    </a>\n",
      "    |\n",
      "    <a href=\"https://bazaar.launchpad.net/%7Eleonardr/beautifulsoup/bs4/view/head:/CHANGELOG\">\n",
      "     Changelog\n",
      "    </a>\n",
      "    |\n",
      "    <a href=\"https://groups.google.com/forum/?fromgroups#!forum/beautifulsoup\">\n",
      "     Discussion group\n",
      "    </a>\n",
      "    |\n",
      "    <a href=\"zine/\">\n",
      "     Zine\n",
      "    </a>\n",
      "    ]\n",
      "   </p>\n",
      "   <p>\n",
      "    <small>\n",
      "     If you use Beautiful Soup as part of your work, please consider a\n",
      "     <a href=\"https://tidelift.com/subscription/pkg/pypi-beautifulsoup4?utm_source=pypi-beautifulsoup4&amp;utm_medium=referral&amp;utm_campaign=website\">\n",
      "      Tidelift subscription\n",
      "     </a>\n",
      "     . This will support many of the free software projects your organization depends on, not just Beautiful Soup.\n",
      "    </small>\n",
      "   </p>\n",
      "   <p>\n",
      "    If Beautiful Soup is useful to you on a personal level, you might like to read\n",
      "    <a href=\"zine/\">\n",
      "     <i>\n",
      "      Tool Safety\n",
      "     </i>\n",
      "    </a>\n",
      "    , a short zine I wrote about what I learned about software development from working on Beautiful Soup. Thanks!\n",
      "   </p>\n",
      "  </div>\n",
      "  <p>\n",
      "   <i>\n",
      "    If you have questions, send them to\n",
      "    <a href=\"https://groups.google.com/forum/?fromgroups#!forum/beautifulsoup\">\n",
      "     the discussion\n",
      "group\n",
      "    </a>\n",
      "    . If you find a bug,\n",
      "    <a href=\"https://bugs.launchpad.net/beautifulsoup/\">\n",
      "     file it\n",
      "    </a>\n",
      "    .\n",
      "   </i>\n",
      "  </p>\n",
      "  <p>\n",
      "   Beautiful Soup is a Python library designed for quick turnaround\n",
      "projects like screen-scraping. Three features make it powerful:\n",
      "  </p>\n",
      "  <ol>\n",
      "   <li>\n",
      "    Beautiful Soup provides a few simple methods and Pythonic idioms\n",
      "for navigating, searching, and modifying a parse tree: a toolkit for\n",
      "dissecting a document and extracting what you need. It doesn't take\n",
      "much code to write an application\n",
      "   </li>\n",
      "   <li>\n",
      "    Beautiful Soup automatically converts incoming documents to\n",
      "Unicode and outgoing documents to UTF-8. You don't have to think\n",
      "about encodings, unless the document doesn't specify an encoding and\n",
      "Beautiful Soup can't detect one. Then you just have to specify the\n",
      "original encoding.\n",
      "   </li>\n",
      "   <li>\n",
      "    Beautiful Soup sits on top of popular Python parsers like\n",
      "    <a href=\"http://lxml.de/\">\n",
      "     lxml\n",
      "    </a>\n",
      "    and\n",
      "    <a href=\"http://code.google.com/p/html5lib/\">\n",
      "     html5lib\n",
      "    </a>\n",
      "    , allowing you\n",
      "to try out different parsing strategies or trade speed for\n",
      "flexibility.\n",
      "   </li>\n",
      "  </ol>\n",
      "  <p>\n",
      "   Beautiful Soup parses anything you give it, and does the tree\n",
      "traversal stuff for you. You can tell it \"Find all the links\", or\n",
      "\"Find all the links of class\n",
      "   <tt>\n",
      "    externalLink\n",
      "   </tt>\n",
      "   \", or \"Find all the\n",
      "links whose urls match \"foo.com\", or \"Find the table heading that's\n",
      "got bold text, then give me that text.\"\n",
      "  </p>\n",
      "  <p>\n",
      "   Valuable data that was once locked up in poorly-designed websites\n",
      "is now within your reach. Projects that would have taken hours take\n",
      "only minutes with Beautiful Soup.\n",
      "  </p>\n",
      "  <p>\n",
      "   Interested?\n",
      "   <a href=\"bs4/doc/\">\n",
      "    Read more.\n",
      "   </a>\n",
      "   <a name=\"Download\">\n",
      "    <h2>\n",
      "     Download Beautiful Soup\n",
      "    </h2>\n",
      "   </a>\n",
      "  </p>\n",
      "  <p>\n",
      "   The current release is\n",
      "   <a href=\"bs4/download/\">\n",
      "    Beautiful Soup\n",
      "4.7.1\n",
      "   </a>\n",
      "   (January 6, 2019). You can install Beautiful Soup 4 with\n",
      "   <code>\n",
      "    pip install beautifulsoup4\n",
      "   </code>\n",
      "   .\n",
      "  </p>\n",
      "  <p>\n",
      "   In Debian and Ubuntu, Beautiful Soup is available as the\n",
      "   <code>\n",
      "    python-bs4\n",
      "   </code>\n",
      "   package (for Python 2) or the\n",
      "   <code>\n",
      "    python3-bs4\n",
      "   </code>\n",
      "   package (for Python 3). In Fedora it's\n",
      "available as the\n",
      "   <code>\n",
      "    python-beautifulsoup4\n",
      "   </code>\n",
      "   package.\n",
      "  </p>\n",
      "  <p>\n",
      "   Beautiful Soup is licensed under the MIT license, so you can also\n",
      "download the tarball, drop the\n",
      "   <code>\n",
      "    bs4/\n",
      "   </code>\n",
      "   directory into almost\n",
      "any Python application (or into your library path) and start using it\n",
      "immediately. (If you want to do this under Python 3, you will need to\n",
      "manually convert the code using\n",
      "   <code>\n",
      "    2to3\n",
      "   </code>\n",
      "   .)\n",
      "  </p>\n",
      "  <p>\n",
      "   Beautiful Soup 4 works on both Python 2 (2.7+) and Python 3.\n",
      "  </p>\n",
      "  <h3>\n",
      "   Beautiful Soup 3\n",
      "  </h3>\n",
      "  <p>\n",
      "   Beautiful Soup 3 was the official release line of Beautiful Soup\n",
      "from May 2006 to March 2012. It is considered stable, and only\n",
      "critical security bugs will be fixed.\n",
      "   <a href=\"http://www.crummy.com/software/BeautifulSoup/bs3/documentation.html\">\n",
      "    Here's\n",
      "the Beautiful Soup 3 documentation.\n",
      "   </a>\n",
      "  </p>\n",
      "  <p>\n",
      "   Beautiful Soup 3 works only under Python 2.x. It is licensed under\n",
      "the same license as Python itself.\n",
      "  </p>\n",
      "  <p>\n",
      "   The current release of Beautiful Soup 3 is\n",
      "   <a href=\"download/3.x/BeautifulSoup-3.2.1.tar.gz\">\n",
      "    3.2.1\n",
      "   </a>\n",
      "   (February 16,\n",
      "2012). You can install Beautiful Soup 3 with\n",
      "   <code>\n",
      "    pip install\n",
      "BeautifulSoup\n",
      "   </code>\n",
      "   . It's also available as\n",
      "   <code>\n",
      "    python-beautifulsoup\n",
      "   </code>\n",
      "   in Debian and Ubuntu, and as\n",
      "   <code>\n",
      "    python-BeautifulSoup\n",
      "   </code>\n",
      "   in Fedora.\n",
      "  </p>\n",
      "  <p>\n",
      "   You can also download the tarball and use\n",
      "   <code>\n",
      "    BeautifulSoup.py\n",
      "   </code>\n",
      "   in your project directly.\n",
      "   <a name=\"HallOfFame\">\n",
      "    <h2>\n",
      "     Hall of Fame\n",
      "    </h2>\n",
      "   </a>\n",
      "  </p>\n",
      "  <p>\n",
      "   Over the years, Beautiful Soup has been used in hundreds of\n",
      "different projects. There's no way I can list them all, but I want to\n",
      "highlight a few high-profile projects. Beautiful Soup isn't what makes\n",
      "these projects interesting, but it did make their completion easier:\n",
      "  </p>\n",
      "  <ul>\n",
      "   <li>\n",
      "    <a href=\"http://www.nytimes.com/2007/10/25/arts/design/25vide.html\">\n",
      "     \"Movable\n",
      " Type\"\n",
      "    </a>\n",
      "    , a work of digital art on display in the lobby of the New\n",
      " York Times building, uses Beautiful Soup to scrape news feeds.\n",
      "   </li>\n",
      "   <li>\n",
      "    Reddit uses Beautiful Soup to\n",
      "    <a href=\"https://github.com/reddit/reddit/blob/85f9cff3e2ab9bb8f19b96acd8da4ebacc079f04/r2/r2/lib/media.py\">\n",
      "     parse\n",
      "a page that's been linked to and find a representative image\n",
      "    </a>\n",
      "    .\n",
      "   </li>\n",
      "   <li>\n",
      "    Alexander Harrowell uses Beautiful Soup to\n",
      "    <a href=\"http://www.harrowell.org.uk/viktormap.html\">\n",
      "     track the business\n",
      " activities\n",
      "    </a>\n",
      "    of an arms merchant.\n",
      "   </li>\n",
      "   <li>\n",
      "    The developers of Python itself used Beautiful Soup to\n",
      "    <a href=\"http://svn.python.org/view/tracker/importer/\">\n",
      "     migrate the Python\n",
      "bug tracker from Sourceforge to Roundup\n",
      "    </a>\n",
      "    .\n",
      "   </li>\n",
      "   <li>\n",
      "    The\n",
      "    <a href=\"http://www2.ljworld.com/\">\n",
      "     Lawrence Journal-World\n",
      "    </a>\n",
      "    uses Beautiful Soup to\n",
      "    <a href=\"http://www.b-list.org/weblog/2010/nov/02/news-done-broke/\">\n",
      "     gather\n",
      "statewide election results\n",
      "    </a>\n",
      "    .\n",
      "   </li>\n",
      "   <li>\n",
      "    The\n",
      "    <a href=\"http://esrl.noaa.gov/gsd/fab/\">\n",
      "     NOAA's Forecast\n",
      "Applications Branch\n",
      "    </a>\n",
      "    uses Beautiful Soup in\n",
      "    <a href=\"http://laps.noaa.gov/topograbber/\">\n",
      "     TopoGrabber\n",
      "    </a>\n",
      "    , a script for\n",
      "downloading \"high resolution USGS datasets.\"\n",
      "   </li>\n",
      "  </ul>\n",
      "  <p>\n",
      "   If you've used Beautiful Soup in a project you'd like me to know\n",
      "about, please do send email to me or\n",
      "   <a href=\"http://groups.google.com/group/beautifulsoup/\">\n",
      "    the discussion\n",
      "group\n",
      "   </a>\n",
      "   .\n",
      "  </p>\n",
      "  <h2>\n",
      "   Development\n",
      "  </h2>\n",
      "  <p>\n",
      "   Development happens at\n",
      "   <a href=\"https://launchpad.net/beautifulsoup\">\n",
      "    Launchpad\n",
      "   </a>\n",
      "   . You can\n",
      "   <a href=\"https://code.launchpad.net/beautifulsoup/\">\n",
      "    get the source\n",
      "code\n",
      "   </a>\n",
      "   or\n",
      "   <a href=\"https://bugs.launchpad.net/beautifulsoup/\">\n",
      "    file\n",
      "bugs\n",
      "   </a>\n",
      "   .\n",
      "  </p>\n",
      "  <hr/>\n",
      "  <table>\n",
      "   <tr>\n",
      "    <td valign=\"top\">\n",
      "     <p>\n",
      "      This document (\n",
      "      <a href=\"/source/software/BeautifulSoup/index.bhtml\">\n",
      "       source\n",
      "      </a>\n",
      "      ) is part of Crummy, the webspace of\n",
      "      <a href=\"/self/\">\n",
      "       Leonard Richardson\n",
      "      </a>\n",
      "      (\n",
      "      <a href=\"/self/contact.html\">\n",
      "       contact information\n",
      "      </a>\n",
      "      ). It was last modified on Monday, January 07 2019, 00:54:05 Nowhere Standard Time and last built on Thursday, June 27 2019, 04:00:02 Nowhere Standard Time.\n",
      "     </p>\n",
      "     <p>\n",
      "     </p>\n",
      "     <table class=\"licenseText\">\n",
      "      <tr>\n",
      "       <td>\n",
      "        <a href=\"http://creativecommons.org/licenses/by-sa/2.0/\">\n",
      "         <img border=\"0\" src=\"/nb//resources/img/somerights20.jpg\"/>\n",
      "        </a>\n",
      "       </td>\n",
      "       <td valign=\"top\">\n",
      "        Crummy is © 1996-2019 Leonard Richardson. Unless otherwise noted, all text licensed under a\n",
      "        <a href=\"http://creativecommons.org/licenses/by-sa/2.0/\">\n",
      "         Creative Commons License\n",
      "        </a>\n",
      "        .\n",
      "       </td>\n",
      "      </tr>\n",
      "     </table>\n",
      "     <!--<rdf:RDF xmlns=\"http://web.resource.org/cc/\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\"><Work rdf:about=\"http://www.crummy.com/\"><dc:title>Crummy: The Site</dc:title><dc:rights><Agent><dc:title>Crummy: the Site</dc:title></Agent></dc:rights><dc:format>text/html</dc:format><license rdf:resource=http://creativecommons.org/licenses/by-sa/2.0//></Work><License rdf:about=\"http://creativecommons.org/licenses/by-sa/2.0/\"></License></rdf:RDF>-->\n",
      "    </td>\n",
      "    <td valign=\"top\">\n",
      "     <p>\n",
      "      <b>\n",
      "       Document tree:\n",
      "      </b>\n",
      "     </p>\n",
      "     <dl>\n",
      "      <dd>\n",
      "       <a href=\"http://www.crummy.com/\">\n",
      "        http://www.crummy.com/\n",
      "       </a>\n",
      "       <dl>\n",
      "        <dd>\n",
      "         <a href=\"http://www.crummy.com/software/\">\n",
      "          software/\n",
      "         </a>\n",
      "         <dl>\n",
      "          <dd>\n",
      "           <a href=\"http://www.crummy.com/software/BeautifulSoup/\">\n",
      "            BeautifulSoup/\n",
      "           </a>\n",
      "          </dd>\n",
      "         </dl>\n",
      "        </dd>\n",
      "       </dl>\n",
      "      </dd>\n",
      "     </dl>\n",
      "     Site Search:\n",
      "     <form action=\"/search/\" method=\"get\">\n",
      "      <input maxlength=\"255\" name=\"q\" type=\"text\" value=\"\"/>\n",
      "     </form>\n",
      "    </td>\n",
      "   </tr>\n",
      "  </table>\n",
      " </body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.crummy.com/software/BeautifulSoup/\"\n",
    "r = requests.get(url) # Package the request, send the request and catch the response: r\n",
    "html_doc = r.text #return html as a string, Extracts the response as html: html_doc\n",
    "soup = BeautifulSoup(html_doc) ## Create a BeautifulSoup object from the HTML: soup\n",
    "print(soup.prettify()) #prettify: indent html etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6.2 title and text from *BeautifulSoup*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<title>Beautiful Soup: We called him Tortoise because he taught us.</title>\n",
      "\n",
      "\n",
      "\n",
      "Beautiful Soup: We called him Tortoise because he taught us.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "You didn't write that awful page. You're just trying to get some\n",
      "data out of it. Beautiful Soup is here to help. Since 2004, it's been\n",
      "saving programmers hours or days of work on quick-turnaround\n",
      "screen scraping projects.\n",
      "\n",
      "Beautiful Soup\n",
      "\"A tremendous boon.\" -- Python411 Podcast\n",
      "[ Download | Documentation | Hall of Fame | Source | Changelog | Discussion group  | Zine ]\n",
      "If you use Beautiful Soup as part of your work, please consider a Tidelift subscription. This will support many of the free software projects your organization depends on, not just Beautiful Soup.\n",
      "If Beautiful Soup is useful to you on a personal level, you might like to read Tool Safety, a short zine I wrote about what I learned about software development from working on Beautiful Soup. Thanks!\n",
      "\n",
      "If you have questions, send them to the discussion\n",
      "group. If you find a bug, file it.\n",
      "Beautiful Soup is a Python library designed for quick turnaround\n",
      "projects like screen-scraping. Three features make it powerful:\n",
      "\n",
      "\n",
      "Beautiful Soup provides a few simple methods and Pythonic idioms\n",
      "for navigating, searching, and modifying a parse tree: a toolkit for\n",
      "dissecting a document and extracting what you need. It doesn't take\n",
      "much code to write an application\n",
      "\n",
      "Beautiful Soup automatically converts incoming documents to\n",
      "Unicode and outgoing documents to UTF-8. You don't have to think\n",
      "about encodings, unless the document doesn't specify an encoding and\n",
      "Beautiful Soup can't detect one. Then you just have to specify the\n",
      "original encoding.\n",
      "\n",
      "Beautiful Soup sits on top of popular Python parsers like lxml and html5lib, allowing you\n",
      "to try out different parsing strategies or trade speed for\n",
      "flexibility.\n",
      "\n",
      "\n",
      "Beautiful Soup parses anything you give it, and does the tree\n",
      "traversal stuff for you. You can tell it \"Find all the links\", or\n",
      "\"Find all the links of class externalLink\", or \"Find all the\n",
      "links whose urls match \"foo.com\", or \"Find the table heading that's\n",
      "got bold text, then give me that text.\"\n",
      "\n",
      "Valuable data that was once locked up in poorly-designed websites\n",
      "is now within your reach. Projects that would have taken hours take\n",
      "only minutes with Beautiful Soup.\n",
      "\n",
      "Interested? Read more.\n",
      "Download Beautiful Soup\n",
      "The current release is Beautiful Soup\n",
      "4.7.1 (January 6, 2019). You can install Beautiful Soup 4 with\n",
      "pip install beautifulsoup4.\n",
      "\n",
      "In Debian and Ubuntu, Beautiful Soup is available as the\n",
      "python-bs4 package (for Python 2) or the\n",
      "python3-bs4 package (for Python 3). In Fedora it's\n",
      "available as the python-beautifulsoup4 package.\n",
      "\n",
      "Beautiful Soup is licensed under the MIT license, so you can also\n",
      "download the tarball, drop the bs4/ directory into almost\n",
      "any Python application (or into your library path) and start using it\n",
      "immediately. (If you want to do this under Python 3, you will need to\n",
      "manually convert the code using 2to3.)\n",
      "\n",
      "Beautiful Soup 4 works on both Python 2 (2.7+) and Python 3.\n",
      "\n",
      "Beautiful Soup 3\n",
      "Beautiful Soup 3 was the official release line of Beautiful Soup\n",
      "from May 2006 to March 2012. It is considered stable, and only\n",
      "critical security bugs will be fixed. Here's\n",
      "the Beautiful Soup 3 documentation.\n",
      "Beautiful Soup 3 works only under Python 2.x. It is licensed under\n",
      "the same license as Python itself.\n",
      "\n",
      "The current release of Beautiful Soup 3 is 3.2.1 (February 16,\n",
      "2012). You can install Beautiful Soup 3 with pip install\n",
      "BeautifulSoup. It's also available as\n",
      "python-beautifulsoup in Debian and Ubuntu, and as\n",
      "python-BeautifulSoup in Fedora.\n",
      "\n",
      "You can also download the tarball and use\n",
      "BeautifulSoup.py in your project directly.\n",
      "\n",
      "\n",
      "Hall of Fame\n",
      "Over the years, Beautiful Soup has been used in hundreds of\n",
      "different projects. There's no way I can list them all, but I want to\n",
      "highlight a few high-profile projects. Beautiful Soup isn't what makes\n",
      "these projects interesting, but it did make their completion easier:\n",
      "\n",
      "\n",
      "\"Movable\n",
      " Type\", a work of digital art on display in the lobby of the New\n",
      " York Times building, uses Beautiful Soup to scrape news feeds.\n",
      "\n",
      "Reddit uses Beautiful Soup to parse\n",
      "a page that's been linked to and find a representative image.\n",
      "\n",
      "Alexander Harrowell uses Beautiful Soup to track the business\n",
      " activities of an arms merchant.\n",
      "\n",
      "The developers of Python itself used Beautiful Soup to migrate the Python\n",
      "bug tracker from Sourceforge to Roundup.\n",
      "\n",
      "The Lawrence Journal-World\n",
      "uses Beautiful Soup to gather\n",
      "statewide election results.\n",
      "\n",
      "The NOAA's Forecast\n",
      "Applications Branch uses Beautiful Soup in TopoGrabber, a script for\n",
      "downloading \"high resolution USGS datasets.\"\n",
      "\n",
      "\n",
      "If you've used Beautiful Soup in a project you'd like me to know\n",
      "about, please do send email to me or the discussion\n",
      "group.\n",
      "\n",
      "Development\n",
      "Development happens at Launchpad. You can get the source\n",
      "code or file\n",
      "bugs.\n",
      "This document (source) is part of Crummy, the webspace of Leonard Richardson (contact information). It was last modified on Monday, January 07 2019, 00:54:05 Nowhere Standard Time and last built on Thursday, June 27 2019, 04:00:02 Nowhere Standard Time.Crummy is © 1996-2019 Leonard Richardson. Unless otherwise noted, all text licensed under a Creative Commons License.Document tree:\n",
      "http://www.crummy.com/software/BeautifulSoup/\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Site Search:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the title of Guido's webpage: guido_title\n",
    "crummy_title = soup.title\n",
    "\n",
    "# Print the title of Guido's webpage to the shell\n",
    "print(crummy_title)\n",
    "\n",
    "# Get Guido's text: guido_text\n",
    "crammy_text = soup.text\n",
    "\n",
    "# Print Guido's text to the shell\n",
    "print(crammy_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6.2 hyperlinks from *BeautifulSoup*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<title>Guido's Personal Home Page</title>\n",
      "pics.html\n",
      "pics.html\n",
      "http://www.washingtonpost.com/wp-srv/business/longterm/microsoft/stories/1998/raymond120398.htm\n",
      "http://metalab.unc.edu/Dave/Dr-Fun/df200004/df20000406.jpg\n",
      "http://neopythonic.blogspot.com/2016/04/kings-day-speech.html\n",
      "http://www.python.org\n",
      "Resume.html\n",
      "Publications.html\n",
      "bio.html\n",
      "http://legacy.python.org/doc/essays/\n",
      "http://legacy.python.org/doc/essays/ppt/\n",
      "interviews.html\n",
      "pics.html\n",
      "http://neopythonic.blogspot.com\n",
      "http://www.artima.com/weblogs/index.jsp?blogger=12088\n",
      "https://twitter.com/gvanrossum\n",
      "http://www.dropbox.com\n",
      "Resume.html\n",
      "http://groups.google.com/groups?q=comp.lang.python\n",
      "http://stackoverflow.com\n",
      "guido.au\n",
      "http://legacy.python.org/doc/essays/\n",
      "images/license.jpg\n",
      "http://www.cnpbagwell.com/audio-faq\n",
      "http://sox.sourceforge.net/\n",
      "images/internetdog.gif\n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Specify url\n",
    "url = 'https://www.python.org/~guido/'\n",
    "\n",
    "# Package the request, send the request and catch the response: r\n",
    "r = requests.get(url)\n",
    "\n",
    "# Extracts the response as html: html_doc\n",
    "html_doc = r.text\n",
    "\n",
    "# create a BeautifulSoup object from the HTML: soup\n",
    "soup = BeautifulSoup(html_doc)\n",
    "\n",
    "# Print the title of Guido's webpage\n",
    "print(soup.title)\n",
    "\n",
    "# Find all 'a' tags (which define hyperlinks): a_tags (hyperlinks defined in anchar tag <a> )\n",
    "a_tags = soup.find_all(\"a\")\n",
    "\n",
    "# Print the URLs to the shell\n",
    "for link in a_tags:\n",
    "    print(link.get(\"href\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 API (Application Programming Interface)\n",
    "\n",
    "An API is a set of protocols or routines for building and interacting with software applications.    \n",
    "A bunch of code to allow two software to communicate with each other.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7.1 Connecting to an API with Python   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"Title\":\"The Social Network\",\"Year\":\"2010\",\"Rated\":\"PG-13\",\"Released\":\"01 Oct 2010\",\"Runtime\":\"120 min\",\"Genre\":\"Biography, Drama\",\"Director\":\"David Fincher\",\"Writer\":\"Aaron Sorkin (screenplay), Ben Mezrich (book)\",\"Actors\":\"Jesse Eisenberg, Rooney Mara, Bryan Barter, Dustin Fitzsimons\",\"Plot\":\"Harvard student Mark Zuckerberg creates the social networking site. That would become known as Facebook but is later sued by two brothers who claimed he stole their idea, and the co-founder who was later squeezed out of the business.\",\"Language\":\"English, French\",\"Country\":\"USA\",\"Awards\":\"Won 3 Oscars. Another 165 wins & 168 nominations.\",\"Poster\":\"https://m.media-amazon.com/images/M/MV5BOGUyZDUxZjEtMmIzMC00MzlmLTg4MGItZWJmMzBhZjE0Mjc1XkEyXkFqcGdeQXVyMTMxODk2OTU@._V1_SX300.jpg\",\"Ratings\":[{\"Source\":\"Internet Movie Database\",\"Value\":\"7.7/10\"},{\"Source\":\"Rotten Tomatoes\",\"Value\":\"95%\"},{\"Source\":\"Metacritic\",\"Value\":\"95/100\"}],\"Metascore\":\"95\",\"imdbRating\":\"7.7\",\"imdbVotes\":\"569,263\",\"imdbID\":\"tt1285016\",\"Type\":\"movie\",\"DVD\":\"11 Jan 2011\",\"BoxOffice\":\"$96,400,000\",\"Production\":\"Columbia Pictures\",\"Website\":\"http://www.thesocialnetwork-movie.com/\",\"Response\":\"True\"}\n",
      "Title: The Social Network\n",
      "Year: 2010\n",
      "Rated: PG-13\n",
      "Released: 01 Oct 2010\n",
      "Runtime: 120 min\n",
      "Genre: Biography, Drama\n",
      "Director: David Fincher\n",
      "Writer: Aaron Sorkin (screenplay), Ben Mezrich (book)\n",
      "Actors: Jesse Eisenberg, Rooney Mara, Bryan Barter, Dustin Fitzsimons\n",
      "Plot: Harvard student Mark Zuckerberg creates the social networking site. That would become known as Facebook but is later sued by two brothers who claimed he stole their idea, and the co-founder who was later squeezed out of the business.\n",
      "Language: English, French\n",
      "Country: USA\n",
      "Awards: Won 3 Oscars. Another 165 wins & 168 nominations.\n",
      "Poster: https://m.media-amazon.com/images/M/MV5BOGUyZDUxZjEtMmIzMC00MzlmLTg4MGItZWJmMzBhZjE0Mjc1XkEyXkFqcGdeQXVyMTMxODk2OTU@._V1_SX300.jpg\n",
      "Ratings: [{'Source': 'Internet Movie Database', 'Value': '7.7/10'}, {'Source': 'Rotten Tomatoes', 'Value': '95%'}, {'Source': 'Metacritic', 'Value': '95/100'}]\n",
      "Metascore: 95\n",
      "imdbRating: 7.7\n",
      "imdbVotes: 569,263\n",
      "imdbID: tt1285016\n",
      "Type: movie\n",
      "DVD: 11 Jan 2011\n",
      "BoxOffice: $96,400,000\n",
      "Production: Columbia Pictures\n",
      "Website: http://www.thesocialnetwork-movie.com/\n",
      "Response: True\n"
     ]
    }
   ],
   "source": [
    "url = \"http://www.omdbapi.com/?apikey=72bc447a&t=the+social+network\" #string starts with \"?\" is a query string, ask about movie with the title \"Hackers\", you also need an api key.\n",
    "r = requests.get(url) #package and send the request to the url, describe the API enquiry and catch the response.\n",
    "json_data = r.json() #the response object r has a method called json which is a json decoder, this method will return a dictionary\n",
    "print(r.text) #print out the text, r has a method called text\n",
    "for key,value in json_data.items():\n",
    "    print(key + \":\", value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p class=\"mw-empty-elt\">\n",
      "</p>\n",
      "\n",
      "<p><b>Pizza</b> (<small>Italian: </small><span title=\"Representation in the International Phonetic Alphabet (IPA)\">[ˈpittsa]</span>, <small>Neapolitan: </small><span title=\"Representation in the International Phonetic Alphabet (IPA)\">[ˈpittsə]</span>) is a savory dish of Italian origin, consisting of a usually round, flattened base of leavened wheat-based dough topped with tomatoes, cheese, and various other ingredients (anchovies, olives, meat, etc.) baked at a high temperature, traditionally in a wood-fired oven. In formal settings, like a restaurant, pizza is eaten with knife and fork, but in casual settings it is cut into wedges to be eaten while held in the hand. Small pizzas are sometimes called pizzettas.\n",
      "</p><p>The term <i>pizza</i> was first recorded in the 10th century in a Latin manuscript from the Southern Italian town of Gaeta in Lazio, on the border with Campania. Modern pizza was invented in Naples, and the dish and its variants have since become popular in many countries. It has become one of the most popular foods in the world and a common fast food item in Europe and North America, available at pizzerias (restaurants specializing in pizza),  restaurants offering Mediterranean cuisine, and via pizza delivery. Many companies sell ready-baked frozen pizzas to be reheated in an ordinary home oven.\n",
      "</p><p>The <i>Associazione Verace Pizza Napoletana</i> (lit. True Neapolitan Pizza Association) is a non-profit organization founded in 1984 with headquarters in Naples that aims to promote traditional Neapolitan pizza. In 2009, upon Italy's request, Neapolitan pizza was registered with the European Union as a Traditional Speciality Guaranteed dish, and in 2017 the art of its making was included on UNESCO's list of intangible cultural heritage.</p>\n"
     ]
    }
   ],
   "source": [
    "#Wikipedia for Pizza\n",
    "# Assign URL to variable: url\n",
    "url = \"https://en.wikipedia.org/w/api.php?action=query&prop=extracts&format=json&exintro=&titles=pizza\"\n",
    "\n",
    "# Package the request, send the request and catch the response: r\n",
    "r = requests.get(url)\n",
    "\n",
    "# Decode the JSON data into a dictionary: json_data\n",
    "json_data = r.json()\n",
    "\n",
    "\n",
    "# Print the Wikipedia page extract\n",
    "pizza_extract = json_data['query']['pages']['24768']['extract']\n",
    "print(pizza_extract)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7.2 JSON (JavaScript Object Notation)\n",
    "\n",
    "A standard form of transferring data through API is the JSON file format.   \n",
    "Real-time server to browser communications.   \n",
    "JSON file format resemble of a dictionary: \"key\" :\"value\". Keys are strings enclosed by quotation mark while value can be string, integer, array or even JSON (therefore it is nested)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load JSON files\n",
    "\n",
    "import json\n",
    "\n",
    "with open(\"../python_basics/data/sample.json\",\"r\") as json_file:\n",
    "    json_data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(json_data) #the type of the file is actually a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fruit: Apple\n",
      "size: Large\n",
      "color: Red\n"
     ]
    }
   ],
   "source": [
    "for key,value in json_data.items(): #print out JSON file\n",
    "    print(key + \":\", value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fruit:  Apple\n",
      "size:  Large\n",
      "color:  Red\n"
     ]
    }
   ],
   "source": [
    "for key in json_data.keys(): #same\n",
    "    print(key + ': ', json_data[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. REST APIs\n",
    "\n",
    "REST stands for REpresentational State Transfer. It originally had a more abstract meaning, but has come to be a shorthand name for web sites that act a bit like python functions, taking as inputs values for certain parameters and producing outputs in the form of a long text string.      \n",
    "\n",
    "API stands for Application Programming Interface. An API specifies how an external program (an application program) can request that a program perform certain computations.      \n",
    "\n",
    "Putting the two together, a REST API specifies how external programs can make HTTP requests to a web site in order to request that some computation be carried out and data returned as output. When a website is designed to accept requests generated by other computer programs, and produce outputs to be consumed by other programs, it is sometimes called a web service, as opposed to a web site which produces output meant for humans to consume in a web browser.      \n",
    "\n",
    "Of course, a web browser is just a computer program, so all requests to web sites come from other computer programs. But usually a browser requests data from a web site in order to display it directly to a human user of the browser.     \n",
    "\n",
    "Prior to the development of REST APIs, there were other ways that computer programs made remote requests to other computers on a network, asking them to perform some computation or retrieve some data. Those other techniques are still in use. REST APIs are particularly convenient, however, both for students and for others, because it is easy to see what is going on in a request and a response between two computers, and thus it is easier to debug.       \n",
    "\n",
    "We will examine a common pattern used in REST APIs, where there is a base URL that defines an “endpoint”, and then additional information is appended to the URL as query parameters, and the response comes back not as HTML but as a format called JSON. Along the way, we will see some functions in python modules that are helpful for constructing the URLS and for processing the JSON formatted response.      \n",
    "\n",
    "In a REST API, the client or application program– the kind of program you will be writing– makes an HTTP request that includes information about what kind of request it is making. Web sites are free to define whatever format they want for how the request should be formatted. This chapter covers a particularly common and particularly simple format, where the request information is encoded right in the URL. This is convenient, because if something goes wrong, we can debug by copying the URL into a browser and see what happens when it tries to visit that URL.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'requests.models.Response'>\n",
      "[{\"word\":\"money\",\"score\":4422,\"numSyllables\":2},{\"word\":\"honey\",\"score\":1208,\"numSyllables\":2},{\"word\":\"sunny\",\"score\":717,\"numSyllables\":2},{\"word\":\"\n",
      "https://api.datamuse.com/words?rel_rhy=funny\n",
      "------\n",
      "<class 'list'>\n",
      "---first item in the list---\n",
      "{'word': 'money', 'score': 4422, 'numSyllables': 2}\n",
      "---the whole list, pretty printed---\n",
      "[\n",
      "  {\n",
      "    \"word\": \"money\",\n",
      "    \"score\": 4422,\n",
      "    \"numSyllables\": 2\n",
      "  },\n",
      "  {\n",
      "    \"word\": \"honey\",\n",
      "    \"score\": 1208,\n",
      "    \"numSyllables\": 2\n",
      "  },\n",
      "  {\n",
      "    \"word\": \"sunny\",\n",
      "    \"score\": 717,\n",
      "    \"numSyllables\": 2\n",
      "  },\n",
      "  {\n",
      "    \"word\": \"bunny\",\n",
      "    \"score\": 703,\n",
      "    \"numSyllables\": 2\n",
      "  },\n",
      "  {\n",
      "    \"word\": \"blini\",\n",
      "    \"score\": 614,\n",
      "    \"numSyllables\": 2\n",
      "  },\n",
      "  {\n",
      "    \"word\": \"gunny\",\n",
      "    \"score\": 452,\n",
      "    \"numSyllables\": 2\n",
      "  },\n",
      "  {\n",
      "    \"word\": \"tunny\",\n",
      "    \"score\": 301,\n",
      "    \"numSyllables\": 2\n",
      "  },\n",
      "  {\n",
      "    \"word\": \"sonny\",\n",
      "    \"score\": 286,\n",
      "    \"numSyllables\": 2\n",
      "  },\n",
      "  {\n",
      "    \"word\": \"dunny\",\n",
      "    \"score\": 245,\n",
      "    \"numSyllables\": 2\n",
      "  },\n",
      "  {\n",
      "    \"word\": \"runny\",\n",
      "    \"score\": 227,\n",
      "    \"numSyllables\": 2\n",
      "  },\n",
      "  {\n",
      "    \"word\": \"thunny\",\n",
      "    \"score\": 222,\n",
      "    \"numSyllables\": 2\n",
      "  },\n",
      "  {\n",
      "    \"word\": \"aknee\",\n",
      "    \"score\": 179,\n",
      "    \"numSyllables\": 2\n",
      "  },\n",
      "  {\n",
      "    \"word\": \"squinny\",\n",
      "    \"score\": 170,\n",
      "    \"numSyllables\": 2\n",
      "  },\n",
      "  {\n",
      "    \"word\": \"fiat money\",\n",
      "    \"score\": 160,\n",
      "    \"numSyllables\": 4\n",
      "  },\n",
      "  {\n",
      "    \"word\": \"gunnie\",\n",
      "    \"score\": 156,\n",
      "    \"numSyllables\": 2\n",
      "  },\n",
      "  {\n",
      "    \"word\": \"blood money\",\n",
      "    \"score\": 152,\n",
      "    \"numSyllables\": 3\n",
      "  },\n",
      "  {\n",
      "    \"word\": \"squiny\",\n",
      "    \"score\": 151,\n",
      "    \"numSyllables\": 2\n",
      "  },\n",
      "  {\n",
      "    \"word\": \"tunney\",\n",
      "    \"score\": 119,\n",
      "    \"numSyllables\": 2\n",
      "  },\n",
      "  {\n",
      "    \"word\": \"spinny\",\n",
      "    \"score\": 117,\n",
      "    \"numSyllables\": 2\n",
      "  },\n",
      "  {\n",
      "    \"word\": \"pin money\",\n",
      "    \"score\": 107,\n",
      "    \"numSyllables\": 3\n",
      "  },\n",
      "  {\n",
      "    \"word\": \"easy money\",\n",
      "    \"score\": 66,\n",
      "    \"numSyllables\": 4\n",
      "  },\n",
      "  {\n",
      "    \"word\": \"smart money\",\n",
      "    \"score\": 66,\n",
      "    \"numSyllables\": 3\n",
      "  },\n",
      "  {\n",
      "    \"word\": \"earnest money\",\n",
      "    \"score\": 62,\n",
      "    \"numSyllables\": 4\n",
      "  },\n",
      "  {\n",
      "    \"word\": \"easter bunny\",\n",
      "    \"score\": 56,\n",
      "    \"numSyllables\": 4\n",
      "  },\n",
      "  {\n",
      "    \"word\": \"paper money\",\n",
      "    \"score\": 54,\n",
      "    \"numSyllables\": 4\n",
      "  },\n",
      "  {\n",
      "    \"word\": \"pocket money\",\n",
      "    \"score\": 47,\n",
      "    \"numSyllables\": 4\n",
      "  },\n",
      "  {\n",
      "    \"word\": \"folding money\",\n",
      "    \"score\": 46,\n",
      "    \"numSyllables\": 4\n",
      "  },\n",
      "  {\n",
      "    \"word\": \"conscience money\",\n",
      "    \"score\": 41,\n",
      "    \"numSyllables\": 4\n",
      "  },\n",
      "  {\n",
      "    \"word\": \"hush money\",\n",
      "    \"score\": 40,\n",
      "    \"numSyllables\": 3\n",
      "  },\n",
      "  {\n",
      "    \"word\": \"prize money\",\n",
      "    \"score\": 37,\n",
      "    \"numSyllables\": 3\n",
      "  },\n",
      "  {\n",
      "    \"word\": \"amount of money\",\n",
      "    \"score\": 33,\n",
      "    \"numSyllables\": 5\n",
      "  },\n",
      "  {\n",
      "    \"word\": \"for love or money\",\n",
      "    \"score\": 32,\n",
      "    \"numSyllables\": 5\n",
      "  },\n",
      "  {\n",
      "    \"word\": \"tight money\",\n",
      "    \"score\": 32,\n",
      "    \"numSyllables\": 3\n",
      "  },\n",
      "  {\n",
      "    \"word\": \"ship money\",\n",
      "    \"score\": 30,\n",
      "    \"numSyllables\": 3\n",
      "  },\n",
      "  {\n",
      "    \"word\": \"metal money\",\n",
      "    \"score\": 27,\n",
      "    \"numSyllables\": 4\n",
      "  },\n",
      "  {\n",
      "    \"word\": \"sum of money\",\n",
      "    \"score\": 23,\n",
      "    \"numSyllables\": 4\n",
      "  },\n",
      "  {\n",
      "    \"word\": \"entrance money\",\n",
      "    \"score\": 22,\n",
      "    \"numSyllables\": 4\n",
      "  },\n",
      "  {\n",
      "    \"word\": \"cheap money\",\n",
      "    \"score\": 21,\n",
      "    \"numSyllables\": 3\n",
      "  },\n",
      "  {\n",
      "    \"word\": \"spending money\",\n",
      "    \"score\": 21,\n",
      "    \"numSyllables\": 4\n",
      "  },\n",
      "  {\n",
      "    \"word\": \"token money\",\n",
      "    \"score\": 21,\n",
      "    \"numSyllables\": 4\n",
      "  },\n",
      "  {\n",
      "    \"word\": \"waste of money\",\n",
      "    \"score\": 19,\n",
      "    \"numSyllables\": 4\n",
      "  },\n",
      "  {\n",
      "    \"word\": \"ransom money\",\n",
      "    \"score\": 18,\n",
      "    \"numSyllables\": 4\n",
      "  },\n",
      "  {\n",
      "    \"word\": \"hearth money\",\n",
      "    \"score\": 14,\n",
      "    \"numSyllables\": 3\n",
      "  },\n",
      "  {\n",
      "    \"word\": \"munni\",\n",
      "    \"score\": 14,\n",
      "    \"numSyllables\": 2\n",
      "  },\n",
      "  {\n",
      "    \"word\": \"bunnie\",\n",
      "    \"score\": 2,\n",
      "    \"numSyllables\": 2\n",
      "  },\n",
      "  {\n",
      "    \"word\": \"euromoney\",\n",
      "    \"score\": 2,\n",
      "    \"numSyllables\": 4\n",
      "  },\n",
      "  {\n",
      "    \"word\": \"smartmoney\",\n",
      "    \"score\": 2,\n",
      "    \"numSyllables\": 3\n",
      "  },\n",
      "  {\n",
      "    \"word\": \"anyone he\",\n",
      "    \"numSyllables\": 4\n",
      "  },\n",
      "  {\n",
      "    \"word\": \"begun he\",\n",
      "    \"numSyllables\": 3\n",
      "  },\n",
      "  {\n",
      "    \"word\": \"bunney\",\n",
      "    \"numSyllables\": 2\n",
      "  },\n",
      "  {\n",
      "    \"word\": \"ca ne\",\n",
      "    \"numSyllables\": 2\n",
      "  },\n",
      "  {\n",
      "    \"word\": \"done he\",\n",
      "    \"numSyllables\": 2\n",
      "  },\n",
      "  {\n",
      "    \"word\": \"donne e\",\n",
      "    \"numSyllables\": 2\n",
      "  },\n",
      "  {\n",
      "    \"word\": \"everyone he\",\n",
      "    \"numSyllables\": 4\n",
      "  },\n",
      "  {\n",
      "    \"word\": \"fun he\",\n",
      "    \"numSyllables\": 2\n",
      "  },\n",
      "  {\n",
      "    \"word\": \"grandson he\",\n",
      "    \"numSyllables\": 3\n",
      "  },\n",
      "  {\n",
      "    \"word\": \"gun he\",\n",
      "    \"numSyllables\": 2\n",
      "  },\n",
      "  {\n",
      "    \"word\": \"handgun he\",\n",
      "    \"numSyllables\": 3\n",
      "  },\n",
      "  {\n",
      "    \"word\": \"kun hee\",\n",
      "    \"numSyllables\": 2\n",
      "  },\n",
      "  {\n",
      "    \"word\": \"le ne\",\n",
      "    \"numSyllables\": 2\n",
      "  },\n",
      "  {\n",
      "    \"word\": \"lunney\",\n",
      "    \"numSyllables\": 2\n",
      "  },\n",
      "  {\n",
      "    \"word\": \"lunny\",\n",
      "    \"numSyllables\": 2\n",
      "  },\n",
      "  {\n",
      "    \"word\": \"none e\",\n",
      "    \"numSyllables\": 2\n",
      "  },\n",
      "  {\n",
      "    \"word\": \"none he\",\n",
      "    \"numSyllables\": 2\n",
      "  },\n",
      "  {\n",
      "    \"word\": \"nun he\",\n",
      "    \"numSyllables\": 2\n",
      "  },\n",
      "  {\n",
      "    \"word\": \"one he\",\n",
      "    \"numSyllables\": 2\n",
      "  },\n",
      "  {\n",
      "    \"word\": \"one knee\",\n",
      "    \"numSyllables\": 2\n",
      "  },\n",
      "  {\n",
      "    \"word\": \"pun he\",\n",
      "    \"numSyllables\": 2\n",
      "  },\n",
      "  {\n",
      "    \"word\": \"run e\",\n",
      "    \"numSyllables\": 2\n",
      "  },\n",
      "  {\n",
      "    \"word\": \"run he\",\n",
      "    \"numSyllables\": 2\n",
      "  },\n",
      "  {\n",
      "    \"word\": \"shotgun he\",\n",
      "    \"numSyllables\": 3\n",
      "  },\n",
      "  {\n",
      "    \"word\": \"someone e\",\n",
      "    \"numSyllables\": 3\n",
      "  },\n",
      "  {\n",
      "    \"word\": \"someone he\",\n",
      "    \"numSyllables\": 3\n",
      "  },\n",
      "  {\n",
      "    \"word\": \"son e\",\n",
      "    \"numSyllables\": 2\n",
      "  },\n",
      "  {\n",
      "    \"word\": \"son he\",\n",
      "    \"numSyllables\": 2\n",
      "  },\n",
      "  {\n",
      "    \"word\": \"sun e\",\n",
      "    \"numSyllables\": 2\n",
      "  },\n",
      "  {\n",
      "    \"word\": \"sun he\",\n",
      "    \"numSyllables\": 2\n",
      "  },\n",
      "  {\n",
      "    \"word\": \"ton he\",\n",
      "    \"numSyllables\": 2\n",
      "  },\n",
      "  {\n",
      "    \"word\": \"ton ne\",\n",
      "    \"numSyllables\": 2\n",
      "  },\n",
      "  {\n",
      "    \"word\": \"un e\",\n",
      "    \"numSyllables\": 2\n",
      "  },\n",
      "  {\n",
      "    \"word\": \"un he\",\n",
      "    \"numSyllables\": 2\n",
      "  },\n",
      "  {\n",
      "    \"word\": \"un ne\",\n",
      "    \"numSyllables\": 2\n",
      "  },\n",
      "  {\n",
      "    \"word\": \"un ni\",\n",
      "    \"numSyllables\": 2\n",
      "  },\n",
      "  {\n",
      "    \"word\": \"won he\",\n",
      "    \"numSyllables\": 2\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "page = requests.get(\"https://api.datamuse.com/words?rel_rhy=funny\")\n",
    "print(type(page))\n",
    "print(page.text[:150]) # print the first 150 characters\n",
    "print(page.url) # print the url that was fetched\n",
    "print(\"------\")\n",
    "x = page.json() # turn page.text into a python object, same as json.load(page)\n",
    "print(type(x))\n",
    "print(\"---first item in the list---\")\n",
    "print(x[0])\n",
    "print(\"---the whole list, pretty printed---\")\n",
    "print(json.dumps(x, indent=2)) # pretty print the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Generating URL with request\n",
    "\n",
    "The request package will replace special char (space) etc with char suitable for query.     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.google.com/search?q=%22violins+and+guitars%22&tbm=isch\n"
     ]
    }
   ],
   "source": [
    "# e.g. google search for \"violins and guitars\" and in the iamge section\n",
    "d = {'q': '\"violins and guitars\"', 'tbm': 'isch'}\n",
    "results = requests.get(\"https://google.com/search\", params=d)\n",
    "print(results.url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 API documentaion\n",
    "\n",
    "Suppose you have learned about the existence of an API, and want to figure out how to use it. There are five questions that you’ll need to answer.     \n",
    "\n",
    "What is the baseurl (sometimes called endpoint) ?       \n",
    "What keys should you provide in the dictionary you pass for the params parameter?        \n",
    "What values should you provide associated with those keys?         \n",
    "Do you need to authenticate yourself as a licensed user of the API and, if so, how?         \n",
    "What is the structure and meaning of the data that will be provided?       \n",
    "\n",
    "The answers to these questions always depend on design choices made by the service provider who is running the server. Thus, the official documentation they provide will usually be the most helpful. It may also be helpful to find example code snippets or full URLs and responses; if you don’t find that in the documentation, you may want to search for it on Google or StackOverflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['money', 'honey', 'sunny']\n"
     ]
    }
   ],
   "source": [
    "# using a function for API calls\n",
    "# import statements for necessary Python modules\n",
    "import requests\n",
    "\n",
    "def get_rhymes(word):\n",
    "    baseurl = \"https://api.datamuse.com/words\"\n",
    "    params_diction = {} # Set up an empty dictionary for query parameters\n",
    "    params_diction[\"rel_rhy\"] = word\n",
    "    params_diction[\"max\"] = \"3\" # get at most 3 results\n",
    "    resp = requests.get(baseurl, params=params_diction)\n",
    "    # return the top three words\n",
    "    word_ds = resp.json()\n",
    "    return [d['word'] for d in word_ds]\n",
    "    return resp.json() # Return a python object (a list of dictionaries in this case)\n",
    "\n",
    "print(get_rhymes(\"funny\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# requests_with_caching module\n",
    "# dont run\n",
    "import requests\n",
    "import json\n",
    "\n",
    "PERMANENT_CACHE_FNAME = \"permanent_cache.txt\"\n",
    "TEMP_CACHE_FNAME = \"this_page_cache.txt\"\n",
    "\n",
    "def _write_to_file(cache, fname):\n",
    "    with open(fname, 'w') as outfile:\n",
    "        outfile.write(json.dumps(cache, indent=2))\n",
    "\n",
    "def _read_from_file(fname):\n",
    "    try:\n",
    "        with open(fname, 'r') as infile:\n",
    "            res = infile.read()\n",
    "            return json.loads(res)\n",
    "    except:\n",
    "        return {}\n",
    "\n",
    "def add_to_cache(cache_file, cache_key, cache_value):\n",
    "    temp_cache = _read_from_file(cache_file)\n",
    "    temp_cache[cache_key] = cache_value\n",
    "    _write_to_file(temp_cache, cache_file)\n",
    "\n",
    "def clear_cache(cache_file=TEMP_CACHE_FNAME):\n",
    "    _write_to_file({}, cache_file)\n",
    "\n",
    "def make_cache_key(baseurl, params_d, private_keys=[\"api_key\"]):\n",
    "    \"\"\"Makes a long string representing the query.\n",
    "    Alphabetize the keys from the params dictionary so we get the same order each time.\n",
    "    Omit keys with private info.\"\"\"\n",
    "    alphabetized_keys = sorted(params_d.keys())\n",
    "    res = []\n",
    "    for k in alphabetized_keys:\n",
    "        if k not in private_keys:\n",
    "            res.append(\"{}-{}\".format(k, params_d[k]))\n",
    "    return baseurl + \"_\".join(res)\n",
    "\n",
    "def get(baseurl, params={}, private_keys_to_ignore=[\"api_key\"], permanent_cache_file=PERMANENT_CACHE_FNAME, temp_cache_file=TEMP_CACHE_FNAME):\n",
    "    full_url = requests.requestURL(baseurl, params)\n",
    "    cache_key = make_cache_key(baseurl, params, private_keys_to_ignore)\n",
    "    # Load the permanent and page-specific caches from files\n",
    "    permanent_cache = _read_from_file(permanent_cache_file)\n",
    "    temp_cache = _read_from_file(temp_cache_file)\n",
    "    if cache_key in temp_cache:\n",
    "        print(\"found in temp_cache\")\n",
    "        # make a Response object containing text from the change, and the full_url that would have been fetched\n",
    "        return requests.Response(temp_cache[cache_key], full_url)\n",
    "    elif cache_key in permanent_cache:\n",
    "        print(\"found in permanent_cache\")\n",
    "        # make a Response object containing text from the change, and the full_url that would have been fetched\n",
    "        return requests.Response(permanent_cache[cache_key], full_url)\n",
    "    else:\n",
    "        print(\"new; adding to cache\")\n",
    "        # actually request it\n",
    "        resp = requests.get(baseurl, params)\n",
    "        # save it\n",
    "        add_to_cache(temp_cache_file, cache_key, resp.text)\n",
    "        return resp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flikr API, search for photo with tags\n",
    "# dont run\n",
    "# import statements\n",
    "import requests_with_caching\n",
    "import json\n",
    "# import webbrowser\n",
    "\n",
    "# apply for a flickr authentication key at http://www.flickr.com/services/apps/create/apply/?\n",
    "# paste the key (not the secret) as the value of the variable flickr_key\n",
    "flickr_key = 'yourkeyhere'\n",
    "\n",
    "def get_flickr_data(tags_string):\n",
    "    baseurl = \"https://api.flickr.com/services/rest/\"\n",
    "    params_diction = {}\n",
    "    params_diction[\"api_key\"] = flickr_key # from the above global variable\n",
    "    params_diction[\"tags\"] = tags_string # must be a comma separated string to work correctly\n",
    "    params_diction[\"tag_mode\"] = \"all\"\n",
    "    params_diction[\"method\"] = \"flickr.photos.search\"\n",
    "    params_diction[\"per_page\"] = 5\n",
    "    params_diction[\"media\"] = \"photos\"\n",
    "    params_diction[\"format\"] = \"json\"\n",
    "    params_diction[\"nojsoncallback\"] = 1\n",
    "    flickr_resp = requests_with_caching.get(baseurl, params = params_diction, permanent_cache_file=\"flickr_cache.txt\")\n",
    "    # Useful for debugging: print the url! Uncomment the below line to do so.\n",
    "    print(flickr_resp.url) # Paste the result into the browser to check it out...\n",
    "    return flickr_resp.json()\n",
    "\n",
    "result_river_mts = get_flickr_data(\"river,mountains\")\n",
    "\n",
    "# Some code to open up a few photos that are tagged with the mountains and river tags...\n",
    "\n",
    "photos = result_river_mts['photos']['photo']\n",
    "for photo in photos:\n",
    "    owner = photo['owner']\n",
    "    photo_id = photo['id']\n",
    "    url = 'https://www.flickr.com/photos/{}/{}'.format(owner, photo_id)\n",
    "    print(url)\n",
    "    # webbrowser.open(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
